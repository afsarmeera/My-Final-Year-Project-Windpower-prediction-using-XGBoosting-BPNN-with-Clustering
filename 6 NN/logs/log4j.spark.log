18/10/08 06:08:24 INFO SparkContext: Running Spark version 2.1.0
18/10/08 06:08:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/10/08 06:08:25 INFO SecurityManager: Changing view acls to: afsar
18/10/08 06:08:25 INFO SecurityManager: Changing modify acls to: afsar
18/10/08 06:08:25 INFO SecurityManager: Changing view acls groups to: 
18/10/08 06:08:25 INFO SecurityManager: Changing modify acls groups to: 
18/10/08 06:08:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(afsar); groups with view permissions: Set(); users  with modify permissions: Set(afsar); groups with modify permissions: Set()
18/10/08 06:08:26 INFO Utils: Successfully started service 'sparkDriver' on port 54868.
18/10/08 06:08:26 INFO SparkEnv: Registering MapOutputTracker
18/10/08 06:08:27 INFO SparkEnv: Registering BlockManagerMaster
18/10/08 06:08:27 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
18/10/08 06:08:27 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
18/10/08 06:08:27 INFO DiskBlockManager: Created local directory at C:\Users\afsar\AppData\Local\Temp\blockmgr-5d6e79ce-b0c8-45a9-b037-245d9e744a00
18/10/08 06:08:27 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
18/10/08 06:08:27 INFO SparkEnv: Registering OutputCommitCoordinator
18/10/08 06:08:28 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/10/08 06:08:28 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4040
18/10/08 06:08:28 INFO SparkContext: Added JAR file:/B:/R/R-3.5.1/library/sparklyr/java/sparklyr-2.1-2.11.jar at spark://127.0.0.1:54868/jars/sparklyr-2.1-2.11.jar with timestamp 1538959108684
18/10/08 06:08:29 INFO Executor: Starting executor ID driver on host localhost
18/10/08 06:08:29 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54890.
18/10/08 06:08:29 INFO NettyBlockTransferService: Server created on 127.0.0.1:54890
18/10/08 06:08:29 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18/10/08 06:08:29 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 54890, None)
18/10/08 06:08:29 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:54890 with 366.3 MB RAM, BlockManagerId(driver, 127.0.0.1, 54890, None)
18/10/08 06:08:29 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 54890, None)
18/10/08 06:08:29 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 127.0.0.1, 54890, None)
18/10/08 06:08:30 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
18/10/08 06:08:30 INFO SharedState: Warehouse path is 'C:/Users/afsar/AppData/Local/spark/spark-2.1.0-bin-hadoop2.7/tmp/hive'.
18/10/08 06:08:31 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
18/10/08 06:08:33 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
18/10/08 06:08:33 INFO ObjectStore: ObjectStore, initialize called
18/10/08 06:08:34 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
18/10/08 06:08:34 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
18/10/08 06:08:38 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
18/10/08 06:08:39 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
18/10/08 06:08:39 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
18/10/08 06:08:41 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
18/10/08 06:08:41 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
18/10/08 06:08:41 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
18/10/08 06:08:41 INFO ObjectStore: Initialized ObjectStore
18/10/08 06:08:41 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
18/10/08 06:08:42 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
18/10/08 06:08:43 INFO HiveMetaStore: Added admin role in metastore
18/10/08 06:08:43 INFO HiveMetaStore: Added public role in metastore
18/10/08 06:08:43 INFO HiveMetaStore: No user is added in admin role, since config is empty
18/10/08 06:08:43 INFO HiveMetaStore: 0: get_all_databases
18/10/08 06:08:43 INFO audit: ugi=afsar	ip=unknown-ip-addr	cmd=get_all_databases	
18/10/08 06:08:43 INFO HiveMetaStore: 0: get_functions: db=default pat=*
18/10/08 06:08:43 INFO audit: ugi=afsar	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
18/10/08 06:08:43 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
18/10/08 06:08:44 INFO SessionState: Created local directory: C:/Users/afsar/AppData/Local/Temp/6d572f5d-3356-426d-8aa1-2dc3685feebd_resources
18/10/08 06:08:44 INFO SessionState: Created HDFS directory: C:/Users/afsar/AppData/Local/spark/spark-2.1.0-bin-hadoop2.7/tmp/hive/afsar/6d572f5d-3356-426d-8aa1-2dc3685feebd
18/10/08 06:08:44 INFO SessionState: Created local directory: C:/Users/afsar/AppData/Local/spark/spark-2.1.0-bin-hadoop2.7/tmp/hive/6d572f5d-3356-426d-8aa1-2dc3685feebd
18/10/08 06:08:44 INFO SessionState: Created HDFS directory: C:/Users/afsar/AppData/Local/spark/spark-2.1.0-bin-hadoop2.7/tmp/hive/afsar/6d572f5d-3356-426d-8aa1-2dc3685feebd/_tmp_space.db
18/10/08 06:08:44 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is C:/Users/afsar/AppData/Local/spark/spark-2.1.0-bin-hadoop2.7/tmp/hive
18/10/08 06:08:44 INFO HiveMetaStore: 0: get_database: default
18/10/08 06:08:44 INFO audit: ugi=afsar	ip=unknown-ip-addr	cmd=get_database: default	
18/10/08 06:08:44 INFO HiveMetaStore: 0: get_database: global_temp
18/10/08 06:08:44 INFO audit: ugi=afsar	ip=unknown-ip-addr	cmd=get_database: global_temp	
18/10/08 06:08:44 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
18/10/08 06:08:45 INFO SparkSqlParser: Parsing command: SHOW TABLES
18/10/08 06:08:51 INFO HiveMetaStore: 0: get_database: default
18/10/08 06:08:51 INFO audit: ugi=afsar	ip=unknown-ip-addr	cmd=get_database: default	
18/10/08 06:08:51 INFO HiveMetaStore: 0: get_database: default
18/10/08 06:08:51 INFO audit: ugi=afsar	ip=unknown-ip-addr	cmd=get_database: default	
18/10/08 06:08:51 INFO HiveMetaStore: 0: get_tables: db=default pat=*
18/10/08 06:08:51 INFO audit: ugi=afsar	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
18/10/08 06:16:43 INFO SparkSqlParser: Parsing command: SHOW TABLES
18/10/08 06:16:43 INFO HiveMetaStore: 0: get_database: default
18/10/08 06:16:43 INFO audit: ugi=afsar	ip=unknown-ip-addr	cmd=get_database: default	
18/10/08 06:16:43 INFO HiveMetaStore: 0: get_database: default
18/10/08 06:16:43 INFO audit: ugi=afsar	ip=unknown-ip-addr	cmd=get_database: default	
18/10/08 06:16:43 INFO HiveMetaStore: 0: get_tables: db=default pat=*
18/10/08 06:16:43 INFO audit: ugi=afsar	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
18/10/08 06:16:44 INFO CodeGenerator: Code generated in 593.676743 ms
18/10/08 06:16:45 INFO SparkContext: Starting job: collect at utils.scala:44
18/10/08 06:16:45 INFO DAGScheduler: Got job 0 (collect at utils.scala:44) with 1 output partitions
18/10/08 06:16:45 INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:44)
18/10/08 06:16:45 INFO DAGScheduler: Parents of final stage: List()
18/10/08 06:16:45 INFO DAGScheduler: Missing parents: List()
18/10/08 06:16:45 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at map at utils.scala:41), which has no missing parents
18/10/08 06:16:45 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 8.7 KB, free 366.3 MB)
18/10/08 06:16:46 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.6 KB, free 366.3 MB)
18/10/08 06:16:46 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 127.0.0.1:54890 (size: 4.6 KB, free: 366.3 MB)
18/10/08 06:16:46 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:996
18/10/08 06:16:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at map at utils.scala:41)
18/10/08 06:16:46 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
18/10/08 06:16:46 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 6041 bytes)
18/10/08 06:16:46 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
18/10/08 06:16:46 INFO Executor: Fetching spark://127.0.0.1:54868/jars/sparklyr-2.1-2.11.jar with timestamp 1538959108684
18/10/08 06:16:46 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:54868 after 63 ms (0 ms spent in bootstraps)
18/10/08 06:16:46 INFO Utils: Fetching spark://127.0.0.1:54868/jars/sparklyr-2.1-2.11.jar to C:\Users\afsar\AppData\Local\Temp\spark-4ba89fad-a51c-4f12-bd11-8928fa58036f\userFiles-091514ec-f839-4af4-aff2-156621f7c09e\fetchFileTemp1838200067506747622.tmp
18/10/08 06:16:47 INFO Executor: Adding file:/C:/Users/afsar/AppData/Local/Temp/spark-4ba89fad-a51c-4f12-bd11-8928fa58036f/userFiles-091514ec-f839-4af4-aff2-156621f7c09e/sparklyr-2.1-2.11.jar to class loader
18/10/08 06:16:47 INFO CodeGenerator: Code generated in 18.582932 ms
18/10/08 06:16:47 INFO CodeGenerator: Code generated in 16.414727 ms
18/10/08 06:16:47 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1318 bytes result sent to driver
18/10/08 06:16:47 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1121 ms on localhost (executor driver) (1/1)
18/10/08 06:16:47 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
18/10/08 06:16:47 INFO DAGScheduler: ResultStage 0 (collect at utils.scala:44) finished in 1.221 s
18/10/08 06:16:47 INFO DAGScheduler: Job 0 finished: collect at utils.scala:44, took 2.307246 s
18/10/08 06:16:56 INFO ContextCleaner: Cleaned accumulator 0
18/10/08 06:16:56 INFO ContextCleaner: Cleaned accumulator 1
18/10/08 06:16:56 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 127.0.0.1:54890 in memory (size: 4.6 KB, free: 366.3 MB)
18/10/08 06:16:57 INFO SparkSqlParser: Parsing command: z
18/10/08 06:16:57 INFO SparkSqlParser: Parsing command: CACHE TABLE `z`
18/10/08 06:16:57 INFO SparkSqlParser: Parsing command: `z`
18/10/08 06:16:57 INFO CodeGenerator: Code generated in 28.841496 ms
18/10/08 06:16:57 INFO CodeGenerator: Code generated in 14.797169 ms
18/10/08 06:16:58 INFO ContextCleaner: Cleaned accumulator 52
18/10/08 06:16:58 INFO SparkContext: Starting job: sql at <unknown>:0
18/10/08 06:16:58 INFO DAGScheduler: Registering RDD 15 (sql at <unknown>:0)
18/10/08 06:16:58 INFO DAGScheduler: Got job 1 (sql at <unknown>:0) with 1 output partitions
18/10/08 06:16:58 INFO DAGScheduler: Final stage: ResultStage 2 (sql at <unknown>:0)
18/10/08 06:16:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
18/10/08 06:16:58 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)
18/10/08 06:16:58 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[15] at sql at <unknown>:0), which has no missing parents
18/10/08 06:16:58 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 22.4 KB, free 366.3 MB)
18/10/08 06:16:58 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 9.3 KB, free 366.3 MB)
18/10/08 06:16:58 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:54890 (size: 9.3 KB, free: 366.3 MB)
18/10/08 06:16:58 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:996
18/10/08 06:16:58 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[15] at sql at <unknown>:0)
18/10/08 06:16:58 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
18/10/08 06:16:59 WARN TaskSetManager: Stage 1 contains a task of very large size (23312 KB). The maximum recommended task size is 100 KB.
18/10/08 06:16:59 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 23871938 bytes)
18/10/08 06:16:59 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
18/10/08 06:16:59 INFO CodeGenerator: Code generated in 23.535165 ms
18/10/08 06:16:59 INFO CodeGenerator: Code generated in 141.001613 ms
18/10/08 06:17:02 INFO MemoryStore: Block rdd_12_0 stored as values in memory (estimated size 23.5 MB, free 342.8 MB)
18/10/08 06:17:02 INFO BlockManagerInfo: Added rdd_12_0 in memory on 127.0.0.1:54890 (size: 23.5 MB, free: 342.8 MB)
18/10/08 06:17:02 INFO CodeGenerator: Code generated in 9.394873 ms
18/10/08 06:17:02 INFO CodeGenerator: Code generated in 50.78117 ms
18/10/08 06:17:02 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2820 bytes result sent to driver
18/10/08 06:17:02 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 4171 ms on localhost (executor driver) (1/1)
18/10/08 06:17:02 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
18/10/08 06:17:02 INFO DAGScheduler: ShuffleMapStage 1 (sql at <unknown>:0) finished in 4.173 s
18/10/08 06:17:02 INFO DAGScheduler: looking for newly runnable stages
18/10/08 06:17:02 INFO DAGScheduler: running: Set()
18/10/08 06:17:02 INFO DAGScheduler: waiting: Set(ResultStage 2)
18/10/08 06:17:02 INFO DAGScheduler: failed: Set()
18/10/08 06:17:02 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[18] at sql at <unknown>:0), which has no missing parents
18/10/08 06:17:02 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 7.0 KB, free 342.7 MB)
18/10/08 06:17:02 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.7 KB, free 342.7 MB)
18/10/08 06:17:02 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 127.0.0.1:54890 (size: 3.7 KB, free: 342.8 MB)
18/10/08 06:17:02 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:996
18/10/08 06:17:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[18] at sql at <unknown>:0)
18/10/08 06:17:02 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
18/10/08 06:17:02 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 5953 bytes)
18/10/08 06:17:02 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
18/10/08 06:17:02 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
18/10/08 06:17:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 28 ms
18/10/08 06:17:03 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2039 bytes result sent to driver
18/10/08 06:17:03 INFO DAGScheduler: ResultStage 2 (sql at <unknown>:0) finished in 0.124 s
18/10/08 06:17:03 INFO DAGScheduler: Job 1 finished: sql at <unknown>:0, took 4.419944 s
18/10/08 06:17:03 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 124 ms on localhost (executor driver) (1/1)
18/10/08 06:17:03 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
18/10/08 06:17:03 INFO CodeGenerator: Code generated in 10.545946 ms
18/10/08 06:17:03 INFO SparkSqlParser: Parsing command: SELECT count(*) FROM  `z`
18/10/08 06:17:03 INFO SparkContext: Starting job: collect at utils.scala:197
18/10/08 06:17:03 INFO DAGScheduler: Registering RDD 22 (collect at utils.scala:197)
18/10/08 06:17:03 INFO DAGScheduler: Got job 2 (collect at utils.scala:197) with 1 output partitions
18/10/08 06:17:03 INFO DAGScheduler: Final stage: ResultStage 4 (collect at utils.scala:197)
18/10/08 06:17:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
18/10/08 06:17:03 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
18/10/08 06:17:03 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[22] at collect at utils.scala:197), which has no missing parents
18/10/08 06:17:03 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 22.4 KB, free 342.7 MB)
18/10/08 06:17:03 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.3 KB, free 342.7 MB)
18/10/08 06:17:03 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 127.0.0.1:54890 (size: 9.3 KB, free: 342.8 MB)
18/10/08 06:17:03 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:996
18/10/08 06:17:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[22] at collect at utils.scala:197)
18/10/08 06:17:03 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
18/10/08 06:17:03 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 127.0.0.1:54890 in memory (size: 3.7 KB, free: 342.8 MB)
18/10/08 06:17:03 INFO ContextCleaner: Cleaned accumulator 161
18/10/08 06:17:03 WARN TaskSetManager: Stage 3 contains a task of very large size (23312 KB). The maximum recommended task size is 100 KB.
18/10/08 06:17:03 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 23871930 bytes)
18/10/08 06:17:03 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
18/10/08 06:17:04 INFO BlockManager: Found block rdd_12_0 locally
18/10/08 06:17:04 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2171 bytes result sent to driver
18/10/08 06:17:04 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 917 ms on localhost (executor driver) (1/1)
18/10/08 06:17:04 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
18/10/08 06:17:04 INFO DAGScheduler: ShuffleMapStage 3 (collect at utils.scala:197) finished in 0.918 s
18/10/08 06:17:04 INFO DAGScheduler: looking for newly runnable stages
18/10/08 06:17:04 INFO DAGScheduler: running: Set()
18/10/08 06:17:04 INFO DAGScheduler: waiting: Set(ResultStage 4)
18/10/08 06:17:04 INFO DAGScheduler: failed: Set()
18/10/08 06:17:04 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[25] at collect at utils.scala:197), which has no missing parents
18/10/08 06:17:04 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 7.0 KB, free 342.7 MB)
18/10/08 06:17:04 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.7 KB, free 342.7 MB)
18/10/08 06:17:04 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 127.0.0.1:54890 (size: 3.7 KB, free: 342.8 MB)
18/10/08 06:17:04 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:996
18/10/08 06:17:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[25] at collect at utils.scala:197)
18/10/08 06:17:04 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
18/10/08 06:17:04 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 5945 bytes)
18/10/08 06:17:04 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
18/10/08 06:17:04 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
18/10/08 06:17:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
18/10/08 06:17:04 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1873 bytes result sent to driver
18/10/08 06:17:04 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 11 ms on localhost (executor driver) (1/1)
18/10/08 06:17:04 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
18/10/08 06:17:04 INFO DAGScheduler: ResultStage 4 (collect at utils.scala:197) finished in 0.011 s
18/10/08 06:17:04 INFO DAGScheduler: Job 2 finished: collect at utils.scala:197, took 0.983123 s
18/10/08 06:17:04 INFO SparkSqlParser: Parsing command: SELECT *
FROM `z` AS `zzz1`
WHERE (0 = 1)
18/10/08 06:17:04 INFO SparkSqlParser: Parsing command: SHOW TABLES
18/10/08 06:17:04 INFO HiveMetaStore: 0: get_database: default
18/10/08 06:17:04 INFO audit: ugi=afsar	ip=unknown-ip-addr	cmd=get_database: default	
18/10/08 06:17:04 INFO HiveMetaStore: 0: get_database: default
18/10/08 06:17:04 INFO audit: ugi=afsar	ip=unknown-ip-addr	cmd=get_database: default	
18/10/08 06:17:04 INFO HiveMetaStore: 0: get_tables: db=default pat=*
18/10/08 06:17:04 INFO audit: ugi=afsar	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
18/10/08 06:17:04 INFO CodeGenerator: Code generated in 16.703137 ms
18/10/08 06:21:08 INFO SparkSqlParser: Parsing command: SELECT * FROM z LIMIT 5
18/10/08 06:21:08 INFO SparkContext: Starting job: collect at utils.scala:197
18/10/08 06:21:08 INFO DAGScheduler: Got job 3 (collect at utils.scala:197) with 1 output partitions
18/10/08 06:21:08 INFO DAGScheduler: Final stage: ResultStage 5 (collect at utils.scala:197)
18/10/08 06:21:08 INFO DAGScheduler: Parents of final stage: List()
18/10/08 06:21:08 INFO DAGScheduler: Missing parents: List()
18/10/08 06:21:08 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[28] at collect at utils.scala:197), which has no missing parents
18/10/08 06:21:08 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 18.1 KB, free 342.7 MB)
18/10/08 06:21:08 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.4 KB, free 342.7 MB)
18/10/08 06:21:08 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 127.0.0.1:54890 (size: 7.4 KB, free: 342.8 MB)
18/10/08 06:21:08 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:996
18/10/08 06:21:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[28] at collect at utils.scala:197)
18/10/08 06:21:08 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
18/10/08 06:21:08 WARN TaskSetManager: Stage 5 contains a task of very large size (23312 KB). The maximum recommended task size is 100 KB.
18/10/08 06:21:08 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 23871856 bytes)
18/10/08 06:21:08 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
18/10/08 06:21:08 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 127.0.0.1:54890 in memory (size: 3.7 KB, free: 342.8 MB)
18/10/08 06:21:09 INFO BlockManager: Found block rdd_12_0 locally
18/10/08 06:21:09 INFO CodeGenerator: Code generated in 116.57094 ms
18/10/08 06:21:09 WARN Executor: 1 block locks were not released by TID = 5:
[rdd_12_0]
18/10/08 06:21:09 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1965 bytes result sent to driver
18/10/08 06:21:09 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 877 ms on localhost (executor driver) (1/1)
18/10/08 06:21:09 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
18/10/08 06:21:09 INFO DAGScheduler: ResultStage 5 (collect at utils.scala:197) finished in 0.877 s
18/10/08 06:21:09 INFO DAGScheduler: Job 3 finished: collect at utils.scala:197, took 0.901181 s
18/10/08 06:21:09 INFO CodeGenerator: Code generated in 19.393765 ms
18/10/08 06:21:09 INFO SparkSqlParser: Parsing command: SELECT * FROM z LIMIT 5
18/10/08 06:21:10 INFO SparkContext: Starting job: collect at utils.scala:197
18/10/08 06:21:10 INFO DAGScheduler: Got job 4 (collect at utils.scala:197) with 1 output partitions
18/10/08 06:21:10 INFO DAGScheduler: Final stage: ResultStage 6 (collect at utils.scala:197)
18/10/08 06:21:10 INFO DAGScheduler: Parents of final stage: List()
18/10/08 06:21:10 INFO DAGScheduler: Missing parents: List()
18/10/08 06:21:10 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[30] at collect at utils.scala:197), which has no missing parents
18/10/08 06:21:10 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 18.1 KB, free 342.7 MB)
18/10/08 06:21:10 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.4 KB, free 342.7 MB)
18/10/08 06:21:10 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 127.0.0.1:54890 (size: 7.4 KB, free: 342.8 MB)
18/10/08 06:21:10 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:996
18/10/08 06:21:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[30] at collect at utils.scala:197)
18/10/08 06:21:10 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
18/10/08 06:21:10 WARN TaskSetManager: Stage 6 contains a task of very large size (23312 KB). The maximum recommended task size is 100 KB.
18/10/08 06:21:10 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 23871856 bytes)
18/10/08 06:21:10 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
18/10/08 06:21:10 INFO BlockManager: Found block rdd_12_0 locally
18/10/08 06:21:10 WARN Executor: 1 block locks were not released by TID = 6:
[rdd_12_0]
18/10/08 06:21:10 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1807 bytes result sent to driver
18/10/08 06:21:10 INFO DAGScheduler: ResultStage 6 (collect at utils.scala:197) finished in 0.533 s
18/10/08 06:21:10 INFO DAGScheduler: Job 4 finished: collect at utils.scala:197, took 0.547372 s
18/10/08 06:21:10 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 533 ms on localhost (executor driver) (1/1)
18/10/08 06:21:10 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
18/10/08 06:38:30 INFO ContextCleaner: Cleaned accumulator 63
18/10/08 06:38:30 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 127.0.0.1:54890 in memory (size: 7.4 KB, free: 342.8 MB)
18/10/08 06:38:30 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 127.0.0.1:54890 in memory (size: 7.4 KB, free: 342.8 MB)
18/10/08 06:38:30 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 127.0.0.1:54890 in memory (size: 9.3 KB, free: 342.8 MB)
18/10/08 06:38:30 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 127.0.0.1:54890 in memory (size: 9.3 KB, free: 342.8 MB)
18/10/08 06:38:30 INFO ContextCleaner: Cleaned shuffle 0
18/10/08 06:38:30 INFO ContextCleaner: Cleaned accumulator 64
18/10/08 06:38:30 INFO ContextCleaner: Cleaned accumulator 62
18/10/08 06:38:30 INFO ContextCleaner: Cleaned accumulator 61
18/10/08 06:38:30 INFO ContextCleaner: Cleaned accumulator 60
18/10/08 06:38:30 INFO ContextCleaner: Cleaned accumulator 59
18/10/08 06:38:30 INFO ContextCleaner: Cleaned accumulator 58
18/10/08 06:38:30 INFO ContextCleaner: Cleaned accumulator 57
18/10/08 06:38:30 INFO ContextCleaner: Cleaned accumulator 56
18/10/08 06:38:30 INFO ContextCleaner: Cleaned accumulator 55
18/10/08 06:38:30 INFO ContextCleaner: Cleaned accumulator 54
18/10/08 06:38:30 INFO ContextCleaner: Cleaned accumulator 53
18/10/08 06:46:28 INFO SparkContext: Invoking stop() from shutdown hook
18/10/08 06:46:28 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
18/10/08 06:46:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/10/08 06:46:28 INFO MemoryStore: MemoryStore cleared
18/10/08 06:46:28 INFO BlockManager: BlockManager stopped
18/10/08 06:46:28 INFO BlockManagerMaster: BlockManagerMaster stopped
18/10/08 06:46:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
18/10/08 06:46:28 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\afsar\AppData\Local\Temp\spark-4ba89fad-a51c-4f12-bd11-8928fa58036f\userFiles-091514ec-f839-4af4-aff2-156621f7c09e
java.io.IOException: Failed to delete: C:\Users\afsar\AppData\Local\Temp\spark-4ba89fad-a51c-4f12-bd11-8928fa58036f\userFiles-091514ec-f839-4af4-aff2-156621f7c09e
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1010)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:102)
	at org.apache.spark.SparkContext$$anonfun$stop$11.apply$mcV$sp(SparkContext.scala:1842)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1283)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1841)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
18/10/08 06:46:28 INFO SparkContext: Successfully stopped SparkContext
18/10/08 06:46:28 INFO ShutdownHookManager: Shutdown hook called
18/10/08 06:46:28 INFO ShutdownHookManager: Deleting directory C:\Users\afsar\AppData\Local\Temp\spark-4ba89fad-a51c-4f12-bd11-8928fa58036f\userFiles-091514ec-f839-4af4-aff2-156621f7c09e
18/10/08 06:46:28 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\afsar\AppData\Local\Temp\spark-4ba89fad-a51c-4f12-bd11-8928fa58036f\userFiles-091514ec-f839-4af4-aff2-156621f7c09e
java.io.IOException: Failed to delete: C:\Users\afsar\AppData\Local\Temp\spark-4ba89fad-a51c-4f12-bd11-8928fa58036f\userFiles-091514ec-f839-4af4-aff2-156621f7c09e
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1010)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
18/10/08 06:46:28 INFO ShutdownHookManager: Deleting directory C:\Users\afsar\AppData\Local\Temp\spark-4ba89fad-a51c-4f12-bd11-8928fa58036f
18/10/08 06:46:28 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\afsar\AppData\Local\Temp\spark-4ba89fad-a51c-4f12-bd11-8928fa58036f
java.io.IOException: Failed to delete: C:\Users\afsar\AppData\Local\Temp\spark-4ba89fad-a51c-4f12-bd11-8928fa58036f
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1010)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
