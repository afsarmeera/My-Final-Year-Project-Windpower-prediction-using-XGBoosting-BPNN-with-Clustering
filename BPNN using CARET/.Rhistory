na.omit(data)
wss <- sapply(1:k.max,function(k){kmeans(data, k, nstart=50,iter.max = 15 )$tot.withinss})
c<-na.omit(data)
wss <- sapply(1:k.max,function(k){kmeans(c, k, nstart=50,iter.max = 15 )$tot.withinss})
plot(1:k.max, wss,
type="b", pch = 19, frame = FALSE,
xlab="Number of clusters K",
ylab="Total within-clusters sum of squares")
set.seed(20)
data <- marketDTM1
c<-na.omit(data)
clusters <- kmeans(c, 6)
View(c)
marketDTM1$cluster <- as.factor(clusters$cluster)
c$cluster <- as.factor(clusters$cluster)
write.csv(c,"cluster.csv")
t<-c[1]
View(t)
t<-c[1,]
View(t)
t<-c[100,]
View(100)
View(t)
View(RohingyaTerrorReality.Tweets)
View(RohingyaTerrorReality.Tweets)
View(marketTDM)
View(marketTDM)
f<-d[320:320,]
f<-d[50,]
View(d)
df$text[2,]
df$text[2]
h<-df$text[2]
View(h)
b<- sapply(h,function(row) iconv(row, "latin1", "ASCII", sub=""))
aCorpus <- VCorpus(VectorSource(b))   #With this, only 1-Grams are created
aCorpus <- tm_map(aCorpus, stripWhitespace)
aCorpus<- tm_map(aCorpus, removePunctuation)
aCorpus <- tm_map(aCorpus, removeNumbers)
aCorpus <- tm_map(aCorpus, removeWords, stopwords("english"))
aCorpus <- tm_map(aCorpus, content_transformer(tolower))
aCorpus <- tm_map(aCorpus, stemDocument)
FourgramTokenizer <- function(x) {RWeka::NGramTokenizer(x, RWeka::Weka_control(min=4, max=4))}
aTDM <- TermDocumentMatrix(aCorpus, control=list(tokenize=FourgramTokenizer))
s<-as.matrix(aTDM)
write.csv(s,"marketTDM.csv")
write.csv(s,"tdm.csv")
library(readr)
tdm <- read_csv("tdm.csv", col_types = cols_only(X1 = col_guess()))
View(tdm)
b<- sapply(tdm$X1,function(row) iconv(row, "latin1", "ASCII", sub=""))
aCorpus <- VCorpus(VectorSource(b))   #With this, only 1-Grams are created
aCorpus <- tm_map(aCorpus, stripWhitespace)
aCorpus<- tm_map(aCorpus, removePunctuation)
aCorpus <- tm_map(aCorpus, removeNumbers)
aCorpus <- tm_map(aCorpus, removeWords, stopwords("english"))
aCorpus <- tm_map(aCorpus, content_transformer(tolower))
aCorpus <- tm_map(aCorpus, stemDocument)
FourgramTokenizer <- function(x) {RWeka::NGramTokenizer(x, RWeka::Weka_control(min=4, max=4))}
aTDM <- DocumentTermMatrix(aCorpus, control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))
s<-as.matrix(aTDM)
write.csv(s,"DTM1.csv")
library(readr)
DTM1 <- read_csv("DTM1.csv")
View(DTM1)
DTM1
View(DTM1)
library(dplyr)
max = apply(c , 2 , max)
library(readr)
DTM1 <- read_csv("DTM1.csv", col_types = cols(X1 = col_skip()))
View(DTM1)
View(c)
max = apply(c , 2 , max)
min = apply(c, 2 , min)
scaled = as.data.frame(scale(c, center = min, scale = max - min))
scaled = as.data.frame(scale(c, center = min, scale = max - min))
Vie(min)
View(min)
View(max)
library(neuralnet)
set.seed(2)
NN = neuralnet(cluster ~ . ,c, hidden = 3 , linear.output = T,threshold = 0.01 )
NN = neuralnet(cluster ~ .,c, hidden = 3 , linear.output = T,threshold = 0.01 )
NN = neuralnet(cluster~.,c, hidden = 3 , linear.output = T,threshold = 0.01 )
NN = neuralnet(cluster~ .,c, hidden = 3 , linear.output = T,threshold = 0.01 )
r<-as.data.frame(c)
library(neuralnet)
NN = neuralnet(cluster~ .,r, hidden = 3 , linear.output = T,threshold = 0.01 )
training_set = subset(r$cluster, subset = split)
training_set <- subset(r$cluster, subset = split)
training_set <- subset(c$cluster, subset = split)
ncol(c)
NN = neuralnet(cluster~ c[,2:3312],c, hidden = 3 , linear.output = T,threshold = 0.01 )
c[,1:2]
NN = neuralnet(cluster~ c[,2:3100],c, hidden = 3 , linear.output = T,threshold = 0.01 )
r<=c
r<-c
dim(r)
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
set.seed(3233)
svm_Linear <- train(V14 ~., data = training, method = "svmLinear",
trControl=trctrl,
preProcess = c("center", "scale"),
tuneLength = 10)
library(caret)
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
d
write.csv(d,"d.csv")
x_train <- c0[1:100,]
y_train <- c0o[1:100,]
x_test <- c0[1:30,]
require(caret)
x <- cbind(x_train,y_train)
TrainControl <- trainControl( method = "repeatedcv", number = 10, repeats = 4)
TrainControl <- trainControl( method = "repeatedcv", number = 10, repeats = 4)
require(caret)
install.packages("caret", lib="B:/R/R-3.5.1/library")
require(caret)
install.packages("stringi", lib="B:/R/R-3.5.1/library")
require(caret)
x_train <- c0[1:100,]
y_train <- c0o[1:100,]
x_test <- c0[1:30,]
require(caret)
library("caret")
install.packages(stringi)
library("caret")
library(caret)
install.packages("stringi")
install.packages("stringi")
install.packages("stringi")
library(dplyr)
x_train <- c0[1:100,]
y_train <- c0o[1:100,]
x_test <- c0[1:30,]
library("caret")
x <- cbind(x_train,y_train)
TrainControl <- trainControl( method = "repeatedcv", number = 10, repeats = 4)
library(dplyr)
max = apply(z[1:100,] , 2 , max)
min = apply(z[1:100,], 2 , min)
scaled = as.data.frame(scale(z[1:100,], center = min, scale = max - min))
library(neuralnet)
set.seed(2)
NN = neuralnet(X22 ~ X4+X6+X12+X13+X14+X15+X16+X17+X18 ,scaled, hidden = 3 , linear.output = T,threshold = 0.01 )
plot(NN)
predict_testNN = compute(NN, scaled[1:9])
View(predict_testNN)
View(predict_testNN$net.result)
load("B:/Official/text good bad/.RData")
View(c)
d<-stringdist_join(c,b ,
by = "X1_1",
mode = "left",
ignore_case = FALSE,
method = "jw",
max_dist = 99,
distance_col = "dist"
)%>%group_by(X1_1.x)%>%top_n(1, -dist)
library(fuzzyjoin)
library(dplyr)
library(plyr)
d<-stringdist_join(c,b ,
by = "X1_1",
mode = "left",
ignore_case = FALSE,
method = "jw",
max_dist = 99,
distance_col = "dist"
)%>%group_by(X1_1.x)%>%top_n(1, -dist)
d<-stringdist_join(c,b ,
by = "X1_1",
mode = "left",
ignore_case = FALSE,
method = "soundex",
max_dist = 99,
distance_col = "dist"
)%>%group_by(X1_1.x)%>%top_n(1, -dist)
View(d)
load("B:/Official/ME FINAL PROJECT ph I/.RData")
View(RohingyaTerrorReality.Tweets)
library(readr)
c3 <- read_csv("B:/Official/ME FINAL PROJECT ph I/5 Clusters based data/c3.csv")
View(c3)
z<-na.omit(c3)
library(dplyr)
max = apply(z[1:100,] , 2 , max)
library(dplyr)
max = apply(z[1:100,] , 2 , max)
library(readr)
z <- read_csv("B:/Official/ME FINAL PROJECT ph I/5 Clusters based data/c3.csv",
col_types = cols(X1 = col_skip(), X1_1 = col_skip(),
clusters = col_skip()))
View(z)
max = apply(z[1:100,] , 2 , max)
library(readr)
d <- read_csv("C:/Users/afsar/Downloads/d.csv")
View(d)
library(readr)
dataset <- read_csv("F:/dataset.csv")
View(dataset)
colnames(dataset)[names(dataset) == 'X1'] <- 'X1'
d<-stringdist_join(dataset,d ,
by = "X1_1",
mode = "left",
ignore_case = FALSE,
method = "hamming",
max_dist = 0,
distance_col = "dist"
)%>%group_by(X1_1.x)%>%top_n(1, -dist)
library(fuzzyjoin)
library(dplyr)
library(plyr)
d<-stringdist_join(dataset,d ,
by = "X1_1",
mode = "left",
ignore_case = FALSE,
method = "hamming",
max_dist = 0,
distance_col = "dist"
)%>%group_by(X1_1.x)%>%top_n(1, -dist)
library(readr)
dataset <- read_csv("F:/dataset.csv", col_names = FALSE,
col_types = cols_only(factor = col_guess()))
View(dataset)
library(readr)
dataset <- read_csv("F:/dataset.csv", col_names = FALSE,
col_types = cols_only(X1 = col_guess()))
View(dataset)
library(readr)
d <- read_csv("B:/d.csv", col_types = cols_only(X1 = col_guess()))
View(d)
d<-stringdist_join(dataset,d ,
by = "X1_1",
mode = "left",
ignore_case = FALSE,
method = "hamming",
max_dist = 0,
distance_col = "dist"
)%>%group_by(X1_1.x)%>%top_n(1, -dist)
install.packages("sparklyr")
library(sparklyr)
spark_install(version = "2.3.2")
library(sparklyr)
spark_install(version = "2.3.1")
install.packages("rsparkling")
options(rsparkling.sparklingwater.version = B:/H2o/sparkling-water-2.3.15/assembly/build/libs/sparkling-water-assembly_2.11-2.3.15-all.jar)
options(rsparkling.sparklingwater.version = B:/H2o/sparkling-water-2.3.15/assembly/build/libs/sparkling-water-assembly_2.11-2.3.15-all)
options(rsparkling.sparklingwater.version = "B:/H2o/sparkling-water-2.3.15/assembly/build/libs/sparkling-water-assembly_2.11-2.3.15-all.jar")
sc <- spark_connect(master = "local", version = "2.3.1")
config=spark_config()
config=c(config, list("spark.ext.h2o.node.port.base"="55555", "spark.ext.h2o.client.port.base"="44444"))
sc <- spark_connect(master="yarn-client", app_name = "demo", config = config)
sc <- spark_connect(master = "local")
config=spark_config()
config=c(config, list("spark.ext.h2o.node.port.base"="55555", "spark.ext.h2o.client.port.base"="44444"))
sc <- spark_connect(master="yarn-client", app_name = "demo", config = config)
h2o_context(sc)
h2o.init()
h2o.init()
h2o.init()
library(h2o)
h2o.init()
h2o_context(sc)
library(h2o)
library(sparklyr)
library(rsparkling)
h2o_context(sc)
options(rsparkling.sparklingwater.version ="sparkling-water-assembly_2.11-2.3.15-all.jar")
h2o_context(sc)
options(rsparkling.sparklingwater.version =...)
options(rsparkling.sparklingwater.version ="B:/H2o/sparkling-water-2.3.15/assembly/build/libs/sparkling-water-assembly_2.11-2.3.15-all.jar")
h2o_context(sc)
library(h2o)
library(rsparkling)
library(sparklyr)
h2o_context(sc)
sc <- spark_connect(master = "local")
h2o_context(sc)
View(sc)
h2o_context(sc)
library(dplyr)
sc <- spark_connect("local", version = "2.1.0")
sc <- spark_connect("local", version = "2.3.1")
mtcars_tbl <- copy_to(sc, mtcars, "mtcars")
partitions <- mtcars_tbl %>%
filter(hp >= 100) %>%
mutate(cyl8 = cyl == 8) %>%
sdf_partition(training = 0.5, test = 0.5, seed = 1099)
training <- as_h2o_frame(sc, partitions$training, strict_version_check = FALSE)
test <- as_h2o_frame(sc, partitions$test, strict_version_check = FALSE)
spark version()
spark-version()
spark version
options(rsparkling.sparklingwater.version ="C:/Users/afsar/Documents/R/win-library/3.5/sparklyr/java/sparklyr-2.3-2.11.jar")
sc <- spark_connect(master = "local")
h2o_context(sc)
library(rsparkling)
library(sparklyr)
library(dplyr)
library(h2o)
sc <- spark_connect(master = "local")
sc <- spark_connect("local", version = "2.3.1")
sc <- spark_connect("local", version = "2.1.0")
mtcars_tbl <- copy_to(sc, mtcars, "mtcars")
mtcars_tbl <- copy_to(sc, mtcars, "mtcars")
mtcars_tbl <- copy_to(sc, mtcars, "mtcars",overwrite = TRUE)
partitions <- mtcars_tbl %>%
filter(hp >= 100) %>%
mutate(cyl8 = cyl == 8) %>%
sdf_partition(training = 0.5, test = 0.5, seed = 1099)
training <- as_h2o_frame(sc, partitions$training, strict_version_check = FALSE)
options(rsparkling.sparklingwater.version = "2.1.14")
training <- as_h2o_frame(sc, partitions$training, strict_version_check = FALSE)
library(sparklyr)
library(rsparkling)
library(h2o)
h2o.init()
library(sparklyr)
library(dplyr)
library(nycflights13)
library(ggplot2)
sc <- spark_connect(master="local")
detach("package:rsparkling", unload = TRUE)
if ("package:h2o" %in% search()) { detach("package:h2o", unload = TRUE) }
if (isNamespaceLoaded("h2o")){ unloadNamespace("h2o") }
remove.packages("h2o")
install.packages("h2o", type = "source", repos = "https://h2o-release.s3.amazonaws.com/h2o/rel-wright/7/R")
library(sparklyr)
library(dplyr)
library(nycflights13)
library(ggplot2)
sc <- spark_connect(master="local")
conf <- spark_config()
conf$spark.executor.memory <- "7GB"
library(sparklyr)
library(rsparkling)
library(h2o)
h2o.init()
conf <- spark_config()
conf$spark.executor.memory <- "7GB"
conf$spark.memory.fraction <- 0.9
conf$spark.executor.cores <- 2
conf$spark.dynamicAllocation.enabled <- "false"
sc <- spark_connect(master="spark://master-url:7077",
version = "2.3.1",
config = conf,
spark_home = "B:/spark-2.3.1-bin-hadoop2.7")
sc <- spark_connect(master = "local")
sc <- spark_connect(master = "local", version = "2.3.2")
sc <- spark_connect(master = "local", version = "2.3.1")
spark_home_set(path = "B:/spark-2.3.1-bin-hadoop2.7")
Sys.setenv(SPARK_HOME="B:/spark-2.3.1-bin-hadoop2.7")
library(sparklyr)
options(rsparkling.sparklingwater.version = '2.3.15')
options(rsparkling.sparklingwater.location = "B:/H2o/sparkling-water-2.3.15/assembly/build/libs/sparkling-water-assembly_2.11-2.3.15-all.jar")
conf <- spark_config()
conf$spark.executor.memory <- "7GB"
conf$spark.memory.fraction <- 0.9
conf$spark.executor.cores <- 2
conf$spark.dynamicAllocation.enabled <- "false"
sc <- spark_connect(master="spark://master-url:7077",
version = "2.3.1",
config = conf,
spark_home = "B:/spark-2.3.1-bin-hadoop2.7")
install.packages("rsparkling")
library(sparklyr)
library(rsparkling)
library(h2o)
h2o.init()
conf$`sparklyr.cores.local` <- 4
conf$`sparklyr.shell.driver-memory` <- "16G"
conf <- spark_config()
conf$spark.executor.memory <- "7GB"
conf$spark.memory.fraction <- 0.9
conf$spark.executor.cores <- 2
conf$spark.dynamicAllocation.enabled <- "false"
sc <- spark_connect(master="spark://master-url:7077",
version = "2.3.1",
config = conf,
spark_home = "B:/spark-2.3.1-bin-hadoop2.7")
Sys.setenv(SPARK_HOME="B:/spark-2.3.1-bin-hadoop2.7")
Sys.setenv(JAVA_HOME="C:/Java/jre1.8.0_181")
library(sparklyr)
library(rsparkling)
library(h2o)
h2o.init()
conf <- spark_config()
conf$spark.executor.memory <- "7GB"
conf$spark.memory.fraction <- 0.9
conf$spark.executor.cores <- 2
conf$spark.dynamicAllocation.enabled <- "false"
sc <- spark_connect(master = "local",
version = "2.3.1",
config = conf)
Sys.getenv("JAVA_HOME")
spark_home_dir()
spark_installed_versions()
sc <- spark_connect(master = "local", spark_home=spark_home_dir(version = "2.3.1"))
sc
sc <- spark_connect(master = "local",
version = "2.3.1",
config = conf)
Sys.setenv(JAVA_HOME="C:/Java/jdk1.8.0_181")
sc <- spark_connect(master = "local", spark_home=spark_home_dir(version = "2.3.1"))
sc <- spark_connect(master = "local", spark_home=spark_home_dir(version = "2.3.1"),config = list(sparklyr.gateway.address = "127.0.0.1")
)
sc <- spark_connect(master="local")
library(sparklyr)
library(rsparkling)
library(h2o)
h2o.init()
config$sparklyr.gateway.port = 10000
config$sparklyr.gateway.connect.timeout = 1
conf <- spark_config()
conf$spark.executor.memory <- "7GB"
conf$spark.memory.fraction <- 0.9
conf$spark.executor.cores <- 2
conf$spark.dynamicAllocation.enabled <- "false"
conf$`sparklyr.cores.local` <- 4
conf$`sparklyr.shell.driver-memory` <- "16G"
conf$spark.memory.fraction <- 0.9
sc <- spark_connect(master = "local",
version = "2.3.1",
config = conf)
sc <- spark_connect(master = "local[*]", config = list("sparklyr.shell.driver-memory"="2g"))
sc <- spark_connect(master = "local", version = "2.3.0")
sc <- spark_connect(master = "local", version = "2.3.1")
options(rsparkling.sparklingwater.location = "B:/H2o/sparkling-water-2.3.13/assembly/build/libs/sparkling-water-assembly_2.11-2.3.13-all.jar")
options(rsparkling.sparklingwater.version = '2.3.13')
sparklyr
sparklyr
library(sparklyr)
library(rsparkling)
library(h2o)
h2o.init()
sc <- spark_connect(master = "local")
h2o_context(sc)
conf <- spark_config()
conf$spark.executor.memory <- "7GB"
conf$spark.memory.fraction <- 0.9
conf$spark.executor.cores <- 2
conf$spark.dynamicAllocation.enabled <- "false"
sc <- spark_connect(master="spark://master-url:7077",
version = "2.3.1",
config = conf,
spark_home = "B:/spark-2.3.1-bin-hadoop2.7")
sc <- spark_connect(master = "local",
version = "2.3.1",
config = conf)
h2o_context(sc)
mtcars_tbl <- copy_to(sc, mtcars, "mtcars")
library(ggplot2)
library(dplyr)
sc <- spark_connect(master = "local")
iris_tbl <- copy_to(sc, iris, "iris", overwrite = TRUE)
library(sparklyr)
library(dplyr)
library(nycflights13)
library(sparklyr)
library(rsparkling)
library(h2o)
h2o.init()
library(dplyr)
library(nycflights13)
library(ggplot2)
sc <- spark_connect(master = "local")
conf$`sparklyr.cores.local` <- 4
conf$`sparklyr.shell.driver-memory` <- "16G"
conf$spark.memory.fraction <- 0.9
sc <- spark_connect(master = "local",
version = "2.3.1",
config = conf)
sc <- spark_connect(master="local")
flights <- copy_to(sc, flights, "flights")
src_tbls(sc)
flights <- copy_to(sc, flights, "flights")
airlines <- copy_to(sc, airlines, "airlines")
load("B:/Official/ME FINAL PROJECT ph I/.RData")
load("B:/Official/ME FINAL PROJECT ph I/.RData")
library(dplyr)
scaled = as.data.frame(scale(z[1:100,], center = min, scale = max - min))
max = apply(z[1:100,] , 2 , max)
rm(min)
rm(max)
max = apply(z[1:100,] , 2 , max)
min = apply(z[1:100,], 2 , min)
scaled = as.data.frame(scale(z[1:100,], center = min, scale = max - min))
library(neuralnet)
set.seed(2)
NN = neuralnet(X22 ~ X4+X6+X12+X13+X14+X15+X16+X17+X18 ,scaled, hidden = 3 , linear.output = T,threshold = 0.01 )
plot(NN)
predict_testNN = compute(NN, scaled[1:9])
predict_testNN
setwd("B:/Official/ME FINAL PROJECT ph I/6 NN")
nnout<-cbind(predict_testNN,z)
nnout<-cbind(predict_testNN,z[1:100])
nnout<-cbind(predict_testNN,scaled)
write.csv(nnout)
write.csv(nnout,"nnout.csv")
load("B:/Official/ME FINAL PROJECT ph I/8 .Ens/.RData")
write.csv(predict_testNN,"nnout.csv")
require(caret)
x <- cbind(x_train,y_train)
TrainControl <- trainControl( method = "repeatedcv", number = 10, repeats = 4)
model<- train(X22 ~ ., data = x, method = "nnet", trControl = TrainControl,verbose = FALSE)
predicted <- predict(model, x_test)
View(predicted)
ensout<-cbind(x_test,predicted)
setwd("B:/Official/ME FINAL PROJECT ph I/7 Ensemble")
write.csv(ensout,"ensout.csv")
